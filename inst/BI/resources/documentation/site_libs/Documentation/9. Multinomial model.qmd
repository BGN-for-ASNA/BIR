# Multinomial model

## General Principles
To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables, we can use a _Multinomial_ distribution. 

![](https://statstest.b-cdn.net/wp-content/uploads/2020/05/Multinomial-Logistic-Regression-1.jpg)

## Considerations

::: callout-caution 
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).
  
- One way to interpret a multinomial model is to consider that we need to build $K - 1$ linear models, where $K$ is the number of categories. Once we get the linear prediction for each category, we can convert these predictions to probabilities by building a [<span style="color:#0D6EFD">simplex ğŸ›ˆ</span>]{#simplex}. To do this, we convert the regression outputs using the softmax function (see the "nn.softmax" line in the code). 
   
- The intercept captures the difference in the log-odds of the outcome categories; thus, different categories need different intercepts.
  
- On the other hand, as we assume that the effect of each predictor on the outcome is consistent across all categories, the regression coefficients are shared across categories.

- The relationship between the predictor variables and the log-odds of each category is modeled linearly, allowing us to interpret the effect of each predictor on the log-odds of each category. 
:::

## Example
Below is an example code snippet demonstrating Bayesian multinomial model using the Bayesian Inference (BI) package:

::: {.panel-tabset group="language"}
### Python
```python
from main import*
# Simulated data--------------------------------------------------------------------------
# simulate career choices among 500 individuals
N = 500  # number of individuals
income = jnp.array([1, 2, 5])  # expected income of each career
score = 0.5 * income  # scores for each career, based on income

# next line converts scores to probabilities
p = jnp.array(jax.nn.softmax(score))

# now simulate choice
# outcome career holds event type values, not counts
career = bi.dist.categorical(p, shape=N, sample=True)
m.data_on_model = dict(
    income=income,
    career=career
)
d.to_csv('Sim data multinomial.csv')

# Define model ------------------------------------------------
def model(income, career):
    a = dist.normal(0, 1, shape=[2], name='a')
    b = dist.halfnormal(0.5, shape=[1], name='b')
    s_1 = a[0] + b * income[0]
    s_2 = a[1] + b * income[1]
    s_3 = a[0] + b * income[0]
    p = jax.nn.softmax(jnp.stack([s_1[0], s_2[0], s_3[0]]))
    lk("y", Categorical(probs=p[career]), obs=career)

# Run sampler ------------------------------------------------ 
m.run(model)  

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

### R
```R
library(reticulate)
bi <- import("main")

# Setup device ------------------------------------------------
m = bi$bi(platform='cpu')

# Import data ------------------------------------------------
m$data('Sim data multinomial.csv', sep=',') 

keys <- c("income", "career")
income = unique(m$df$income)
income = income[order(income)]
values <- list(as.integer(income), as.integer(m$df$career))
m.data_on_model = py_dict(keys, values, convert = TRUE)
m.data_on_model

# Define model ------------------------------------------------
model <- function(income, career){
  alpha = bi$dist$normal(0, 1, name='alpha', shape = tuple(as.integer(2)))
  beta = bi$dist$halfnormal(0.5, name='beta', shape = tuple(as.integer(1)))
  s_1 = alpha[0] + beta * income[1]
  s_2 = alpha[1] + beta * income[1]
  s_3 = alpha[0] + beta * income[0]
  p = bi$jax$nn$softmax(bi$jnp$stack(list(s_1[0], s_2[0], s_3[0])))
  bi$lk("y", bi$Categorical(probs=p[career]), obs=career)
}

# Run MCMC ------------------------------------------------
m$run(model) 

# Summary ------------------------------------------------
m$sampler$print_summary(0.89)
```
:::

## Mathematical Details
### *Formula*
In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors ğŸ›ˆ</span>]{#prior}. We model the relationship between the predictor variables (X1, X2, ..., Xn) and the categorical outcome variable ($Y_i$) using the following equation:

$$
logit(p_ik) = log(\frac{p_ik}{p_iK})  = Î²_k^T X_i + Î±_k
$$

Where:


- $p_ik$ is the probability of the ğ‘–-th observation being in category ğ‘˜.
  
- $Î²_k$ is the regression coefficients for category ğ‘˜.
  
- $Î±_k$ is the intercept for category ğ‘˜.
  
- $X_i$ is the vector of predictor variables for the ğ‘–-th observation.
  
- The reference category ğ¾ is often chosen to simplify the model (Note that we did not do this in the code).

We can express the Bayesian Multinomial model as follows:

If  $KâˆˆN ,  NâˆˆN , and  Î¸âˆˆK$-simplex , then for  $yâˆˆNK$  such that  $âˆ‘Kk=1yk=N$:


### *Bayesian model*
In Bayesian multinomial modeling, the likelihood function of the data is specified using a multinomial distribution. The multinomial distribution models the counts of outcomes falling into different categories. For an outcome variable 
ğ‘¦ with ğ¾ categories, the multinomial likelihood function is:
$$
Multinomial(y|Î¸)=\frac{N!}{âˆ^K_{k=1}yk!} âˆ_{k=1}^{K} Î¸_{k}^{y_k}
$$

Where:

- $y=(y_1, y_2,â€¦,y_K)$ represents the counts of observations in each of the ğ¾ categories.
  
- $N$ is the total number of observations or trials.
  
- $Î¸=(Î¸_1,Î¸_2,â€¦,Î¸_K)$  is a simplex of category probabilities, with $Î¸_k$ representing the probability of category ğ‘˜.
  
- $\frac{N!}{âˆ^K_{k=1}yk!}$ is the multinomial coefficient that accounts for the number of ways to arrange the observations into the categories. This coefficient ensures that the likelihood function properly accounts for the permutations of the counts across different categories.
- 
## Reference(s)
@mcelreath2018statistical
