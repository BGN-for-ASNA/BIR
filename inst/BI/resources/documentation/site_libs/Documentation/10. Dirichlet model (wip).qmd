
# Dirichlet Model
## General Principles
To model the relationship between a categorical outcome variable with more than two categories and one or more independent variables with [<span style="color:#0D6EFD">overdispersion ðŸ›ˆ</span>]{#overdispersion}, we can use a _Dirichlet_ distribution.

![](https://statstest.b-cdn.net/wp-content/uploads/2020/05/Multinomial-Logistic-Regression-1.jpg)

## Considerations
::: callout-caution 
- We have the same considerations as for the [Multinomial model](7.&#32;Multinomial&#32;model.qmd).

- One major difference from the multinomial model is that the Dirichlet model doesnâ€™t require a simplex.
:::

## Example
```python
# Simulated data ------------------------------
import seaborn as sns
import numpy as np
from jax import random
from jax.nn import softmax
import jax.numpy as jnp
import numpyro as numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS, Predictive

###############################################################################
############ SIMULATING MULTINOMIAL DATA WITH SOFTMAX LINK FUNCTION ###########
def mysoftmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / np.sum(exp_x, axis=0)

K = 3
N = 100
N_obs = 2
sigma_random = 0.6

########################################################
################### Fixed Effect Sim ###################
# a = np.random.normal(0, 1, K)
a = np.array([3, 1, 1])  # Forcing a values

# Factors--------------------------
NY = 4
NV = 8

Y2 = np.full((NV, NY), np.nan) 
means = np.random.normal(0, 1, NY)
offsets = np.random.normal(0, 1, NV)
for i in range(NV):
    for k in range(NY):
        Y2[i, k] = means[k] + offsets[i]

b_individual = np.random.normal(0, 1, (N, K))
mu = b_individual + a

# Declare an empty matrix to fill with data
Y = np.empty((N * N_obs, K))

# Declare an empty vector to fill with IDs
id = []

# Loop over each individual
for i in range(N):
    # Simulate N_obs draws from the multinomial
    Y[i*N_obs:(i+1)*N_obs, :] = np.apply_along_axis(lambda x: np.random.multinomial(100, softmax(x)), 0, mu[i])
    # Assign ID vector
    id += [i] * N_obs

N = N * N_obs
K = K
ni = N
y = jnp.array(Y, dtype=jnp.int32).reshape(N, K)
i_ID = jnp.array(id)

# Define model ----------------------------
from main import *
m = bi()
def model(K, ni, y, i_ID):
    a = normal('a', [K], 0, 1)
    Sigma_individual = exponential('Sigma_individual', [ni], 1)
    L_individual = lkjcholesky('L_individual', [], ni, 1)  # Implies a uniform distribution over correlation matrices
    z_individual = normal('z_individual', [ni, K], 0, 1)
    alpha = random_centered(Sigma_individual, L_individual, z_individual)
    lk = jnp.exp(a + alpha[i_ID])
    sample("y", DirichletMultinomial(lk, int(100)), obs=y)

m.data_on_model = dict(
    K=K,
    ni=ni,
    y=y,
    i_ID=i_ID
)

# Run sampler ------------------------------------------------ 
m.run(model)  

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

## Mathematical Details
### *Formula*

### *Bayesian Model*
In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors ðŸ›ˆ</span>]{#prior}. We can express the Bayesian regression model accounting for prior distribution as follows:

## Reference(s)
@mcelreath2018statistical

