# Varying slopes

## General Principles

To model the relationship between predictor variables and an independent variable while allowing for varying effects across groups or clusters, we use a *Varying slopes* model.

This approach is useful when we expect the relationship between predictors and the independent variable to differ across groups (e.g., different slopes for different subjects, locations, or time periods).This allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling.

## Considerations
::: callout-caution 
-   We have the same considerations as for [12. Varying interceps](12.%20Varying%20intercepts.qmd).

-   The idea is pretty similar to categorical models, where a slope is specified for each category. However, here, we also estimate relationships between different groups. This leads to a different mathematical approach, as to model these relationships between groups, we model a [<span style="color:#0D6EFD"> matrix of covariance ðŸ›ˆ</span>]{#cov}.

-   The covariance matrix requiere a correlation matris distribution which is modeleld using a $LKJcorr$ distribution that hold a parameter $Î·$. $Î·$ is ussually set to 2 to define a weakly informative prior that is skeptical of extreme correlations near âˆ’1 or 1. When we use LKJ- corr(1), the prior is flat over all valid correlation matrices. When the value is greater than 1, then extreme correlations are less likely.

-   The Half-Cauchy distribution is used when modeling the covariance matrix to specify strictly positive values for the diagonal of the covariance matrix, ensuring positive variances.
:::

## Example

Below is an example code snippet demonstrating Bayesian regression with varying effects:

### Simulated data

::: {.panel-tabset group="language"}
## Python
``` python
from main import*
# Setup device------------------------------------------------
m = bi(platform='cpu')
m.data('Sim data multivariatenormal.csv', sep=',') 
m.data.to_model(['cafe', 'wait', 'N_cafes'])

def model(cafe, wait, N_cafes):
    a = dist.normal(5, 2, name = 'a') #Standard Normal Vector 
    b = dist.normal(-1, 0.5, name = 'b') # Standard Normal Vector 
    sigma_cafe = dist.exponential(1, shape=[2], name = 'sigma_cafe')
    sigma = dist.exponential( 1, name = 'sigma')
    Rho = dist.lkj(2, 2, name = 'Rho') # Cholesky Factor
    # Applies the correlation structure between the variables.
    # Scaling the Cholesky Factor by the standard deviations.
    cov = jnp.outer(sigma_cafe, sigma_cafe) * Rho # In a multivariate normal distribution, we not only have correlations (from sr_L), but also individual standard deviations for each variable (from sr_sigma). The diag_pre_multiply operation ensures that each variable's correlation structure is properly scaled by its standard deviation. This makes it possible to account for both correlations and individual variability when generating samples.
    #This operation applies the correlation and standard deviation structure (encoded in scaled_sr_L) to the standard normal variables sr_raw. The result is a sample from a multivariate normal distribution that respects both the correlations between variables and the individual standard deviations.
    a_cafe_b_cafe = dist.multivariatenormal(jnp.stack([a, b]), cov, shape = [N_cafes], name = 'a_cafe')    

    a_cafe, b_cafe = a_cafe_b_cafe[:, 0], a_cafe_b_cafe[:, 1]
    mu = a_cafe[cafe] + b_cafe[cafe] * afternoon
    lk("y", Normal(mu, sigma), obs=wait)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

## R
``` R
```
:::

## Mathematical Details
## Mathematical Details

### *Formula*

We model the relationship between the independent variable $X$ and the outcome variable *Y* with varying intercepts ($\alpha$) and varying slopes ($\beta$) for each group (*k*) using the following equation:

$$
Y_{ik} = \alpha_k + \beta_k X_{ik} + \sigma
$$

Where: 
- $Y_{ik}$ is the outcome variable for observation *i* in group *k*. 
  
-  $X_{ik}$ is the independent variables for observation *i* in group *k*. 
  
-  $\alpha_k$ is the varying intercept for group *k*.
  
-  $\beta_k$ is the varying regression coefficients for group *k*. 

- $\sigma$ is the error term, assumed to be strictly positive.

### *Bayesian model*

We can express the Bayesian regression model accounting for prior distribution as follows:

$$
Y_{ik} \sim \text{Normal}(\mu_{ik} , \sigma)
$$
$$
\mu_{ik} = \alpha_k + \beta_k X_{ik} + \sigma 
$$
$$
\alpha_k \sim Normal(0,1) 
$$
$$
\beta_k \sim Normal(0,1)
$$
$$
\sigma \sim Exponential(0,1)
$$

The varying intercepts slopes ($\alpha_k$) and ($\beta_k$) are modeled using a *Multivariate Normal distribution*:

$$ 
\begin{pmatrix} 
\alpha_k \\ 
\beta_k 
\end{pmatrix} \sim \text{MultivariateNormal}\left( 
\begin{pmatrix} 
0 \\ 
0 
\end{pmatrix}, 
\begin{pmatrix} 
\sigma_\alpha^2 & \sigma_\pi \sigma_{\alpha\rho} \\ 
\sigma_\alpha \sigma_{\pi\rho} & \sigma_\pi 
\end{pmatrix} 
\right) 
$$

Where: 

- $\left(\begin{array}{cc} 0 \\0\end{array}\right)$, is the prior for average intercept.

-  $\left(\begin{array}{cc} \sigma_\alpha^2 & \sigma_\pi\sigma_{\alpha\rho} \\ \sigma_\alpha\sigma_{\pi\rho} & \sigma_\pi \end{array}\right)$ is is the covariance matrix which specifies the variance and covariance of $\alpha_k$ and $\beta_k$, 
  
-  where: - $\sigma_\alpha^2$ The variance of $\alpha_k$. 
  
- $\sigma_\pi^2$ The variance of $\beta_k$. 
  
-  $\sigma_\pi\sigma_{\alpha\rho}$ and $\sigma_\alpha\sigma_{\pi\rho}$ The covariance between $\alpha_k$ and $\beta_k$

For computational reasons, it is often better to implement a [<span style="color:#0D6EFD">centered version of the varying intercept ðŸ›ˆ</span>]{#centerRF}
that is equivalent to the *Multivariate Normal distribution* approach:

$$
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
 \sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
$$

-   Where:

    -   $\sigma_\alpha \sim Exponential(1)$ bewing the prior standard deviation among intercepts.
  
    -   $\sigma_\beta \sim Exponential(1)$ bewing the prior standard deviation among slopes.
  
    -   $L \sim LKJcorr(Î·)$ bewing the prior for the correlation matrix using the [<span style="color:#0D6EFD">Cholesky Factor ðŸ›ˆ</span>]{#chol}.

The full cetered version of the model is thus :

$$
Y_{i} \sim \text{Normal}(\mu_k , \sigma) \\
$$

$$
\mu_k =   \alpha_k + \beta_i X_i \\
$$

$$
\left(\begin{array}{cc} \alpha_k \\ \beta_k\end{array}\right)
 \sim 
\left(\begin{array}{cc} 
\sigma_\alpha\\
\sigma_\pi
\end{array}\right) \circ 
L *
\left(\begin{array}{cc} 
\widehat{\alpha}_k \\
\widehat{\pi}_k
\end{array}\right)
$$

$$
\alpha \sim Normal(0,1)
$$
$$
\beta \sim Normal(0,1)
$$
$$
\sigma_\alpha \sim Exponential(1)
$$
$$
\sigma_\pi \sim Exponential(1)
$$
$$
L \sim LKJcorr(2)
$$

## Notes
::: callout-note 
-   We can apply multivariate model similarly as [chapter 2](/2.%20Multiple%20Regression%20for%20Continuous%20Variables.qmd). In this case, we apply the same principle, but with a covariance matrix of a dimension equal to the number of varying slopes we define. For example, if we want to generate random slopes for $i$ actors in a model with two independent variables $X_1$ and $X_2$, we can define the formula as follows:

$$
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) 
$$

$$
\mu_i =   \alpha_i + \beta_{1i} X_{1i}  + \beta_{1i} X_{2i} 
$$

$$ 
\begin{pmatrix} 
\alpha_{i}\\ 
\beta_{1i}\\ 
\beta_{2i} 
\end{pmatrix} 
\sim \begin{pmatrix} 
\sigma_{\alpha}\\ 
\sigma_{\pi}\\ 
\sigma_{\gamma} 
\end{pmatrix} \circ L \cdot \begin{pmatrix} 
\widehat{\alpha}_{k} \\ 
\widehat{\pi}_{k} \\ 
\widehat{\gamma}_{k} 
\end{pmatrix} 
$$


$$
\sigma_{\alpha} \sim Exponential(1)
$$
$$
\sigma_{\pi} \sim Exponential(1)
$$
$$
\sigma_{\gamma} \sim Exponential(1) 
$$
$$
L \sim LKJcorr(2)
$$

-   We can apply interaction terms similarly as [chapter 3](\3.%20Interaction%20between%20continuous%20variables.qmd).

-   We can apply caterogical variables similarly as [chapter 4](4.%20Categorical%20variable.qmd).

-   We can apply varying slopes with any distribution presented in previous chapters.

-   For more than two varying effects we apply the same principel but with a covariance matrix for each varying effect that are summed to gernerat the varying intercept and slope. For exmaple, if we want to generate random slopes for $i$ actors, and $k$ groups we can define the formula as follow:

$$
p(Y_{i} |\mu_i , \sigma) \sim \text{Normal}(\mu_i , \sigma) \\
$$

$$
\mu_i =   \alpha_i + \beta_{i} X_i 
$$
$$
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
$$
$$
\beta_{i} = \beta + \beta_{actor[i]} + \beta_{group[i]} 
$$

$$
\alpha \sim Normal(0,1)
$$
$$
\beta \sim Normal(0,1) 
$$

$$ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{\text{actor}} 
\end{pmatrix} 
\sim 
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} 
\end{pmatrix} \circ L_a \cdot \begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} 
\end{pmatrix} 
$$


$$
\sigma_{\alpha a} \sim Exponential(1)
$$
$$
\sigma_{\pi a} \sim Exponential(1)
$$
$$
L_{a} \sim LKJcorr(2)
$$

$$ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{\text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} 
\end{pmatrix} 
$$


$$
\sigma_{\alpha g} \sim Exponential(1)
$$
$$
\sigma_{\pi g} \sim Exponential(1) 
$$
$$
L_{g} \sim LKJcorr(2)
$$

-   Bellow the formula and the code snipset for a Binomial multivariate model with interaction between two independent variables $X_1$ and $X_2$ and multiples varying effects for each actor and each group:

$$
p(Y_{i} |n , p_i) \sim \text{Binomial}(n = 1, p_i) \\
$$

$$
logit{p_i}=   \alpha_i + (\beta_{1i}  + \beta_{2i} X_{2i})  X_{1i}
$$
$$
\alpha_i = \alpha + \alpha_{actor[i]} + \alpha_{group[i]}
$$
$$
\beta_{1i} = \beta + \beta_{1 actor[i]} + \beta_{ group[i]}
$$
$$
\beta_{2i} = \beta + \beta_{2 actor[i]} + \beta_{2 group[i]}
$$

$$
\alpha \sim Normal(0,1)
$$
$$
\beta \sim Normal(0,1)
$$

$$ 
\begin{pmatrix} 
\alpha_{\text{actor}} \\ 
\beta_{1 \, \text{actor}} \\ 
\beta_{2 \, \text{actor}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha a} \\ 
\sigma_{\pi a} \\ 
\sigma_{\gamma a} 
\end{pmatrix} \circ L_a \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{ka} \\ 
\widehat{\pi}_{ka} \\ 
\widehat{\gamma}_{ka} 
\end{pmatrix} 
$$


$$
\sigma_{\alpha a} \sim Exponential(1) 
$$

$$
\sigma_{\pi a} \sim Exponential(1) 
$$
$$
\sigma_{\gamma a} \sim Exponential(1) 
$$
$$
L_{a} \sim LKJcorr(2)
$$

$$ 
\begin{pmatrix} 
\alpha_{\text{group}} \\ 
\beta_{1 \, \text{group}} \\ 
\beta_{2 \, \text{group}} 
\end{pmatrix} 
\sim  
\begin{pmatrix} 
\sigma_{\alpha g} \\ 
\sigma_{\pi g} \\ 
\sigma_{\gamma g} 
\end{pmatrix} \circ L_g \cdot 
\begin{pmatrix} 
\widehat{\alpha}_{kg} \\ 
\widehat{\pi}_{kg} \\ 
\widehat{\gamma}_{kg} 
\end{pmatrix} 
$$


$$
\sigma_{\alpha g} \sim Exponential(1)
$$
$$
\sigma_{\pi g} \sim Exponential(1) 
$$
$$
\sigma_{\gamma g} \sim Exponential(1)
$$
$$
L_{g} \sim LKJcorr(2)
$$

```python
from main import*
# Setup device------------------------------------------------
m = bi(platform='cpu')
# Import data
m.read_csv("../data/chimpanzees.csv", sep=";")
m.df["block_id"] = m.df.block
m.df["treatment"] = 1 + m.df.prosoc_left + 2 * m.df.condition
m.data_to_model(['pulled_left', 'treatment', 'actor', 'block_id'])


def model(tid, actor, block_id, L=None, link=False):
    # fixed priors
    g = dist.normal(0, 1, name = 'g', shape = (4,))
    sigma_actor = dist.exponential(1, name = 'sigma_actor', shape = (4,))
    L_Rho_actor = dist.lkjcholesky(4, 2, name = "L_Rho_actor")
    sigma_block = dist.exponential(1, name = "sigma_block", shape = (4,))
    L_Rho_block = dist.lkjcholesky(4, 2, name = "L_Rho_block")

    # adaptive priors - non-centered
    z_actor = dist.normal(0, 1, name = "z_actor", shape = (4,7))
    z_block = dist.normal(0, 1, name = "z_block", shape = (4,3))
    alpha = deterministic(
        "alpha", ((sigma_actor[..., None] * L_Rho_actor) @ z_actor).T
    )
    beta = deterministic(
        "beta", ((sigma_block[..., None] * L_Rho_block) @ z_block).T
    )

    logit_p = g[tid] + alpha[actor, tid] + beta[block_id, tid]
    dist("L", dist.Binomial(logits=logit_p), obs=L)

    # compute ordinary correlation matrixes from Cholesky factors
    if link:
        deterministic("Rho_actor", L_Rho_actor @ L_Rho_actor.T)
        deterministic("Rho_block", L_Rho_block @ L_Rho_block.T)
        deterministic("p", expit(logit_p))

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```
:::
## Reference(s)

@mcelreath2018statistical