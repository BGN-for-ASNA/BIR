# Multiple continuous variables model
## General Principles
To study relationships between multiple continuous variables (e.g., the effect of height and age on weight), we can use a Multiple Regression approach. Essentially, we extend the [Linear Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd) by adding a regression coefficient $\beta$ for each continuous variable. 

![](https://miro.medium.com/v2/resize:fit:786/format:webp/0*dJqdzk1aMo2OQR7O)

## Considerations
::: callout-caution 
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- We need a regression coefficient $\beta$ for each independent variable.

- Model interpretation of the regression coefficients $\beta$ is considered for a fixed value of the other dependent variables' regression coefficients‚Äîi.e., for a given age, a variation of 1 unit in height reflects the value of the regression coefficient $\beta$ for height.
:::

## Example
Below is an example code snippet demonstrating Bayesian multiple regression using the Bayesian Inference (BI) package:

::: {.panel-tabset group="language"}
### Python
```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/Howell1.csv', sep=';') 
m.df = m.df[m.df.age > 18]
m.scale(['weight', 'age'])
m.data_to_model(['weight', 'height', 'age'])

# Define model ------------------------------------------------
def model(height, weight, age):
    alpha = bi.dist.normal(0, 0.5, name = 'alpha')    
    beta1 = bi.dist.normal(0, 0.5, name = 'beta1')
    beta2 = bi.dist.normal(0, 0.5, name = 'beta2')
    sigma = bi.dist.uniform(0,50, name = 'sigma')

    lk("y", Normal(alpha + beta1 * weight + beta2 * age, sigma), obs=height)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)
```

### R
```R
library(reticulate)
# Setup device------------------------------------------------
bi <- import("main")
m = bi$bi(platform='cpu')

# Import data ------------------------------------------------
m$data('../data/Howell1.csv', sep=';') 
m$df = m$df[m$df$age > 18,]
m$scale(list('weight', 'age'))
m$data_to_model(list('weight', 'height', 'age'))

# Define model ------------------------------------------------
model <- function(height, weight, age){
  alpha = bi$dist$normal( 0, 0.5, name = 'a', shape = tuple(as.integer(1)))
  beta1 = bi$dist$normal( 0, 0.5, name = 'b1', shape = tuple(as.integer(1)))
  beta2 = bi$dist$normal(  0, 0.5, name = 'b2', shape = tuple(as.integer(1)))   
  sigma = bi$dist$uniform(0, 50, name = 's', shape = tuple(as.integer(1)))
  bi$lk("y", bi$Normal(alpha + beta1 * weight + beta2 * age, sigma), obs=height)
}

# Run mcmc ------------------------------------------------
m$run(model) 

# Summary ------------------------------------------------
m$sampler$print_summary(0.89)
```
:::


## Mathematical Details
### *Frequentist formulation*
We model the relationship between the independent variables $(X_1, X_2, ..., X_n)$ and the dependent variable _Y_ using the following equation:

$$
ùëå_i = \alpha +\beta_1  ùëã_{1i} + \beta_2  ùëã_{2i} + ... + \beta_n  ùëã_{1n} + \sigma
$$

Where:

- $Y_i$ is the dependent variable for observation *i*.
  
- $\alpha$ is the intercept term.
  
- $X_{1i}$, $X_{2i}$, ..., $X_{1n}$ are the values of the independent variables for observation *i*.
  
- $\beta_1$, $\beta_2$, ..., $\beta_n$ are the regression coefficients.
  
- $\sigma$ is the error term.
  

### *Bayesian formulation*
In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors üõà</span>]{#prior}. We can express the Bayesian model as follows:

$$
ùëå \sim Normal(\alpha + \sum_k^n  \beta_k  X, œÉ¬≤)
$$

$$
\alpha \sim Normal(0,1)
$$

$$
\beta_i \sim Normal(0,1)
$$

$$
œÉ \sim Uniform(0, 50)
$$

Where:

- $Y_i$ is dependent variable for observation *i*. 
  
- $\alpha$ is the prior distribution for the intercept.
  
- $\beta_k$ are the prior distributions for the regression coefficients _k_ distinct regression coefficients.
  
- $X_{1i}$, $X_{2i}$, ..., $X_{1n}$ are the values of the independent variables for observation *i*.
  
- $\sigma$ is the prior distribution for the standard deviation, ensuring it is positive.

## Reference(s)
@mcelreath2018statistical


