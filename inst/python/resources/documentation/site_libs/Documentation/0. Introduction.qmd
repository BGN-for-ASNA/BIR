# Introduction

## 1.1	Model set-up
We define a likelihood (e.g., a mathematical formula that specifies the plausibility of the data). The likelihood has parameters (e.g., adjustable inputs) for which we define priors (e.g., initial plausibility assignment for each possible value of the parameter). Considering a linear regression with an intercept (e.g., $μ$ value when $x$ is at zero, or at the mean if the data is centered), a slope (e.g., $μ$ change value when $x$ is incremented by one unit), and assuming the data is centered (<span style="color:red"> as we will always concider in the next chapters</span>):


<span style="font-size:0.7em;">
<i>
* Toolpit available for each lines of equation
</i>
</span>

[$$y \sim  Normal(μ,σ)
$$](bi\doc\0.%20%Introduction.md "This is the 'main' equation which use a specific distribution to account for uncertainty in the model.")

[$$ μ \sim α + βx
$$](bi\doc\0.%20%Introduction.md "This is the 'likelihood' that combine the differnt terms of the model")

[$$ α \sim Normal(0,1)
$$](bi\doc\0.%20%Introduction.md "This is the prior for the intercept")

[$$ β \sim Normal(0,1)
$$](bi\doc\0.%20%Introduction.md "This is the prior for the slope or regression coefficient")

[$$ σ \sim Uniform(0,1)
$$](bi\doc\0.%20%Introduction.md "This is the prior")


## 1.2 Model fitting

By using probability distributions for parameters, we can better tune the model by describing parameters with '_subequations_' and accounting for _correlated varying effects_, _Gaussian processes_, _measurement error_, and _missing data_.

In addition, we can use _Bayesian updating_ using the _Bayesian theorem_ to 'reshape' the prior distributions by considering every possible combination of values for µ and σ and scoring each combination by its relative plausibility in light of the data. These relative plausibilities are the posterior probabilities of each combination of values µ and σ: the _posterior distributions_. Various techniques can be used to approximate the mathematics that follows from the definition of Bayes' theorem: grid approximation, quadratic approximation, and Markov chain Monte Carlo (_MCMC_).

[$$\frac{likelihood*Priors}{average likelihood}$$](# "This is the Bayesian theorem")


## 1.3	Model 'diagnostic'
The posterior distribution can be described using percentile intervals (_PI_), the highest posterior density interval (_HPDI_), and point estimates. 
We can also sample the posterior distribution and generate _dummy data_, which can help check the model through _observations and p uncertainty propagation on the samples_. In some aspects, it is the opposite of a null model as it represents an expected model.


## 1.4 Link functions
We will see different families of regreessions that have different distribtions. For the moment we just need to know that those different distribtions required _link function (for each specific family we will discuss the corresponding link function):




## Vocabulary
This method evaluate if variable we want to predict -the dependent variable (_Y_)- and the variable(s) that may affect(s)-independent variables (_Xs_)- this dependent variable is

## Conciderations
When implementing Bayesian linear regression with TensorFlow Probability, it's important to consider the following:
- Specifying appropriate prior distributions for the model parameters.
- Choosing an appropriate likelihood function that captures the relationship between the inputs and outputs.
- Selecting an inference method to approximate the posterior distribution over parameters, such as Markov chain Monte Carlo (MCMC) or variational inference.