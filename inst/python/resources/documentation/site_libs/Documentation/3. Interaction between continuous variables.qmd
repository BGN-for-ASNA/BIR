# Interaction terms
## General Principles
To study relationships between two independent continuous variables and their interaction effect on a dependent variable (e.g., temperature and humidity affecting energy consumption), we can use Regression Analysis with Interaction Terms. In this approach, we extend the simple linear regression model to include an interaction term (a multiplication) between the two continuous variables.

Parallel lines indicate that there is no interaction effect, while different slopes suggest that one might be present. Below is the plot for Food x Condiment. The crossed lines on the graph suggest that there is an interaction effect, which the significant p-value for the Food*Condiment term confirms. The graph shows that enjoyment levels are higher for chocolate sauce when the food is ice cream. Conversely, satisfaction levels are higher for mustard when the food is a hot dog. If you put mustard on ice cream or chocolate sauce on hot dogs, you won’t be happy!

![](https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2017/10/interactions_plot_categorical.png?w=576&ssl=1)

## Considerations
::: callout-caution 
- We have the same considerations as for [Regression for continuous variable](1.&#32;Linear&#32;Regression&#32;for&#32;continuous&#32;variable.qmd).

- Model the relationship between Y and R to vary as a function of A. You explicitly model the hypothesis that the slope between Y and R depends—is conditional—upon A.

- For continuous interactions, the intercept becomes the grand mean of the outcome variable. This ease of interpretation alone is a good reason to center predictor variables.

- The interpretation of estimates is more difficult, as the estimate of non-interaction terms becomes the expected change in Y when R increases by one unit and A is at its average value. The estimate of interaction terms represents the expected change in the influence of A on Y when increasing R by one unit and the expected change in the influence of R on Y when increasing A by one unit.

- [<span style="color:#0D6EFD">Triptych 🛈</span>]{#triptych} plots are very handy for understanding the impact of interactions.
  
:::

## Example
Below is an example code snippet demonstrating Bayesian regression with an interaction term between two continuous variables with the Bayesian Inference (BI) package:

::: {.panel-tabset group="language"}
### Python
```python
from main import*

# Setup device------------------------------------------------
m = bi(platform='cpu')

# Import data ------------------------------------------------
m.data('../data/tulips.csv', sep=';') 
m.scale(['blooms', 'water', 'shade'])
m.data_to_model(['blooms', 'water', 'shade'])

# Define model ------------------------------------------------
def model(blooms, shade, water):
    alpha = dist.normal(0.5, 0.25, name='alpha')
    sigma = dist.exponential(1, name='sigma')
    beta1 = dist.normal(0, 0.25, name='beta1')
    beta2 = dist.normal(0, 0.25, name='beta2')
    beta_interaction_ = dist.normal(0, 0.25, name='beta_interaction_')    
    lk("y", Normal(alpha + beta1 * water + beta2 * shade + beta_interaction_ * water * shade, sigma), obs=blooms)

# Run mcmc ------------------------------------------------
m.run(model) 

# Summary ------------------------------------------------
m.sampler.print_summary(0.89)

```

### R
```R
library(reticulate)
bi <- import("main")

# Setup device------------------------------------------------
m = bi$bi(platform='cpu')

# Import data ------------------------------------------------
m$data('../data/tulips.csv', sep=';') 
m$scale(list('blooms', 'water', 'shade'))
m$data_to_model(list('blooms', 'water', 'shade'))

# Define model ------------------------------------------------
model <- function(blooms, water,shade){
  alpha = bi$dist$normal( 0.5, 0.25, name = 'a',shape= tuple(as.integer(1)))
  beta1 = bi$dist$normal( 0,  0.25, name = 'b1',shape= tuple(as.integer(1)))
  beta2 = bi$dist$normal(  0,  0.25, name = 'b2',shape= tuple(as.integer(1)))   
  beta_interaction_ = bi$dist$normal(  0, 0.25, name = 'bint',shape= tuple(as.integer(1))) 
  sigma = bi$dist$uniform(0, 50, name = 's',shape = tuple(as.integer(1)))
  bi$lk("y", bi$Normal(alpha + beta1*water + beta2*shade + beta_interaction_*water*shade, sigma), obs=blooms)
}

# Run mcmc ------------------------------------------------
m$run(model) 

# Summary ------------------------------------------------
m$sampler$print_summary(0.89)
```
:::

## Mathematical Details
## *Frequentist formulation*
We model the relationship between the input features (X1 and X2) and the target variable (Y) using the following equation:
$$
𝑌_i = \alpha + \beta_1𝑋_{1i} + \beta_2𝑋_{2i} + \beta_{interaction}𝑋_{1i}𝑋_{2i} + \sigma
$$

Where:

- $Y_i$ is the dependent variable for observation *i*.
  
- $\alpha$ is the intercept term.
  
- $X_{1i}$ and $X_{2i}$ are the two values of the independent continuous variables for observation *i*.
  
- $\beta_1$ and $\beta_2$ are the regression coefficients for $X_{1i}$ and $X_{2i}$, respectively.
  
- $\beta_{interaction}$ is the regression coefficient for the interaction term $(X_{1i}  X_{2i})$.
  
- $\sigma$ is the error term assumed to be normally distributed.

In this context, the interaction term $X_{1i} * X_{2i}$ captures the joint effect of $X_{1i}$ and $X_{2i}$ on the target variable $Y_i$.

### *Bayesian formulation*
In the Bayesian formulation, we define each parameter with [<span style="color:#0D6EFD">priors 🛈</span>]{#prior}. We can express the Bayesian regression model accounting for prior distribution as follows:

$$
Y \sim Normal(\alpha +  \beta_1  X_{1i}​ + \beta_2  X_{2i}​​ + \beta_{interaction}  X_1{1i} X_{2i}​ ,  \sigma)
$$

$$
\alpha \sim Normal(0,1)
$$

$$
\beta_1 \sim Normal(0,1)
$$

$$
\beta_2 \sim Normal(0,1)
$$

$$
\beta_{interaction} \sim Normal(0,1)
$$

$$
σ \sim Exponential(1)
$$

Where:

- $Y_i$ is dependent variable for observation *i*.
  
- $\alpha$ is the prior distribution for the intercept.
  
- $\beta_1$,  $\beta_2$, and $\beta_{interaction}$ are the prior distributions for the regression coefficients.
  
- $X_{1i}$ and $X_{2i}$ are the two values of the independent continuous variables for observation *i*.
  
- $\sigma$ is the prior distribution for the standard deviation, ensuring it is positive.

## Reference(s)
@mcelreath2018statistical
